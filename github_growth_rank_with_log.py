import requests
import time
import pandas as pd
import os
from datetime import datetime, timedelta
import json
import sys
import random
import logging
from urllib.parse import urlparse
from tqdm import tqdm
from dotenv import load_dotenv

# è¨­å®šè¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[
                        logging.FileHandler("log.txt", encoding='utf-8'),
                        logging.StreamHandler(sys.stdout)
                    ])

# å¾ç’°å¢ƒè®Šæ•¸ç²å– API Keys
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

if not GITHUB_TOKEN:
    logging.error("GITHUB_TOKEN ç’°å¢ƒè®Šæ•¸æœªè¨­å®šã€‚è«‹æª¢æŸ¥æ‚¨çš„ .env æ–‡ä»¶ã€‚")
    sys.exit(1)
if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
    logging.warning("GOOGLE_API_KEY æˆ– GOOGLE_CSE_ID ç’°å¢ƒè®Šæ•¸æœªè¨­å®šã€‚Google æœå°‹å‚™ç”¨æ–¹æ¡ˆå°‡ç„¡æ³•ä½¿ç”¨ã€‚")

GITHUB_HEADERS = {
    'Accept': 'application/vnd.github+json',
    'Authorization': f'token {GITHUB_TOKEN}'
}

SUCCESS_FILE = 'success_symbols.csv'
CSV_FILE = 'github_growth_rank.csv'

def get_top_symbols(limit=100, quote_asset='USDT'):
    """å¾å¹£å®‰ API ç²å–å¸‚å€¼å‰ N åçš„å¹£ç¨®ç¬¦è™Ÿã€‚"""
    url = "https://api.binance.com/api/v3/ticker/24hr"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() # æª¢æŸ¥ HTTP éŒ¯èª¤
        data = response.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"å¾å¹£å®‰ API ç²å–æ•¸æ“šå¤±æ•—: {e}")
        return []

    usdt_pairs = [item for item in data if item['symbol'].endswith(quote_asset)]
    sorted_pairs = sorted(usdt_pairs, key=lambda x: float(x.get('quoteVolume', 0)), reverse=True)
    top_symbols = [item['symbol'][:-len(quote_asset)].lower() for item in sorted_pairs[:limit]]
    return top_symbols

# ä¿®æ”¹é€™å‡½æ•¸ å¦‚æœerroræ˜¯202æˆ–429 å°±æ˜¯èªªåªéœ€ç­‰å¾…å†é‡è©¦å°±èƒ½è·‘çš„å°±é‡è©¦ è‹¥æ˜¯404é‚£ç¨®å°±ç›´æ¥return None
def fetch_with_retry(url, headers=None, max_retries=99999, initial_delay=5, backoff_factor=2, return_headers=False):
    """
    å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ HTTP è«‹æ±‚å‡½æ•¸ã€‚
    å°æ–¼ 202 å’Œ 429 ç‹€æ…‹ç¢¼ï¼Œæœƒé€²è¡Œé‡è©¦ã€‚
    å°æ–¼ 404 åŠå…¶ä»– 4xx ç‹€æ…‹ç¢¼ï¼Œæœƒç›´æ¥è¿”å› Noneã€‚
    æ–°å¢ return_headers åƒæ•¸ï¼Œå¦‚æœç‚º Trueï¼Œå‰‡è¿”å› response.headersã€‚
    """
    for i in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code == 202:  # GitHub API æ•¸æ“šæ­£åœ¨è¨ˆç®—ä¸­
                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)
                logging.info(f"â³ GitHub æ•¸æ“šæ­£åœ¨è¨ˆç®—ä¸­ (202 Accepted)ï¼Œç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦... (ç¬¬ {i+1} æ¬¡é‡è©¦)")
                time.sleep(delay)
                continue
            elif response.status_code == 429:  # è«‹æ±‚éæ–¼é »ç¹
                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 5) # 429 å¯ä»¥çµ¦æ›´é•·çš„å»¶é²
                logging.warning(f"Too Many Requests (429)ï¼Œç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦... (ç¬¬ {i+1} æ¬¡é‡è©¦)")
                time.sleep(delay)
                continue
            elif 400 <= response.status_code < 500: # 4xx å®¢æˆ¶ç«¯éŒ¯èª¤ï¼Œé™¤äº† 429
                if response.status_code == 404:
                    logging.error(f"è«‹æ±‚ {url} å¤±æ•—: è³‡æºæœªæ‰¾åˆ° (404 Not Found)ã€‚ä¸å†é‡è©¦ã€‚")
                else:
                    logging.error(f"è«‹æ±‚ {url} å¤±æ•—: å®¢æˆ¶ç«¯éŒ¯èª¤ {response.status_code}ã€‚ä¸å†é‡è©¦ã€‚")
                return None # å°æ–¼ 4xx éŒ¯èª¤ï¼Œç›´æ¥è¿”å› None

            response.raise_for_status()  # å°æ–¼é 2xx ç‹€æ…‹ç¢¼ï¼ˆé€™è£¡ä¸»è¦æ˜¯ 5xx éŒ¯èª¤ï¼‰ï¼Œæ‹‹å‡º HTTPError

            # æ–°å¢é‚è¼¯ï¼šå¦‚æœ return_headers ç‚º Trueï¼Œå‰‡è¿”å› response.headers
            if return_headers:
                return response.headers

            try:
                return response.json()
            except json.JSONDecodeError as e:
                logging.error(f"è§£æ JSON å¤±æ•—: {e}, URL: {url}, éŸ¿æ‡‰å…§å®¹: {response.text}")
                if i < max_retries - 1:
                    delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)
                    logging.warning(f"JSON è§£æå¤±æ•—ï¼Œé‡è©¦ {url} (ç¬¬ {i+1} æ¬¡), ç­‰å¾… {delay:.2f} ç§’...")
                    time.sleep(delay)
                    continue
                else:
                    logging.error(f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è«‹æ±‚ {url} (JSON è§£æå¤±æ•—)ã€‚")
                    return None

        except requests.exceptions.RequestException as e:
            logging.error(f"è«‹æ±‚ {url} å¤±æ•—: {e}")
            if i < max_retries - 1:
                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)
                logging.warning(f"é‡è©¦ {url} (ç¬¬ {i+1} æ¬¡), ç­‰å¾… {delay:.2f} ç§’...")
                time.sleep(delay)
            else:
                logging.error(f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è«‹æ±‚ {url}ã€‚")
    return None

def get_coingecko_id(symbol):
    """æ ¹æ“šç¬¦è™Ÿç²å– CoinGecko IDã€‚"""
    logging.info("ä½¿ç”¨get_coingecko_idå…ˆç­‰10ç§’é¿å…é™æµ")
    time.sleep(random.uniform(10, 15))
    url = f"https://api.coingecko.com/api/v3/search?query={symbol}"
    # ä¿®æ”¹é»ï¼šä½¿ç”¨ fetch_with_retry
    data = fetch_with_retry(url) # fetch_with_retry æœƒè¿”å› None æˆ– JSON æ•¸æ“š
    if data and data.get('coins'): # ä½¿ç”¨ .get() é¿å… KeyError
        # å˜—è©¦æ‰¾åˆ°ç²¾ç¢ºåŒ¹é…çš„symbolæˆ–id
        for coin in data['coins']:
            if coin['symbol'].lower() == symbol.lower() or coin['id'].lower() == symbol.lower():
                return coin['id']
        # å¦‚æœæ²’æœ‰ç²¾ç¢ºåŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€å€‹çµæœ
        return data['coins'][0]['id']
    logging.error(f"å¾ CoinGecko æœå°‹ ID å¤±æ•—æˆ–ç„¡çµæœ: {symbol}")
    return None

def fetch_coingecko_github_repo_url(coingecko_id):
    """å¾ CoinGecko API ç²å– GitHub å„²å­˜åº«é€£çµã€‚"""
    logging.info("ä½¿ç”¨fetch_coingecko_github_repo_urlå…ˆç­‰10ç§’é¿å…é™æµ")
    time.sleep(random.uniform(10, 15))
    url = f"https://api.coingecko.com/api/v3/coins/{coingecko_id}"
    # ä¿®æ”¹é»ï¼šä½¿ç”¨ fetch_with_retry
    data = fetch_with_retry(url)
    if data and 'links' in data and 'repos_url' in data['links'] and data['links']['repos_url']:
        # å¯èƒ½æœ‰å…©ç¨®è·¯å¾‘
        github_urls = [link for link in data['links']['repos_url']["github"] if link and "github.com" in link]
        if github_urls:
            return github_urls[0] # è¿”å›ç¬¬ä¸€å€‹ GitHub é€£çµ
        else:
            github_urls = [link for link in data['links']['repos_url'] if link and "github.com" in link]
            if github_urls:
                return github_urls[0] # è¿”å›ç¬¬ä¸€å€‹ GitHub é€£çµ
    logging.warning(f"âŒ CoinGecko æ‰¾ä¸åˆ° {coingecko_id} çš„ GitHub repo é€£çµã€‚URL: {url}")
    return None

def Google_Search_github_repo(query, limit=5):
    """ä½¿ç”¨ Google Custom Search API æœå°‹ GitHub å„²å­˜åº«é€£çµã€‚"""
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        logging.warning("Google API Keys æœªè¨­å®šï¼Œè·³é Google æœå°‹ã€‚")
        return None

    search_url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}&q={query}&num={limit}"
    logging.info("é¿å…è¢«googleé™æµ å»¶é²10ç§’å†ç”¨googleæœå°‹")
    time.sleep(10) # Keep the delay as it's a specific requirement for Google Search

    # Use fetch_with_retry for the Google Custom Search API call
    data = fetch_with_retry(search_url)

    if data and 'items' in data:
        for item in data['items']:
            link = item.get('link')
            if link and "github.com" in link and "/tree/" not in link and "/blob/" not in link and "/wiki/" not in link and "/topics/" not in link:
                # å˜—è©¦éæ¿¾æ‰é repo æ ¹ç›®éŒ„çš„é€£çµ
                # ä¸¦ä¸”ç¢ºä¿ä¸æ˜¯ Gist æˆ– Pages é€™ç¨®
                parsed_url = urlparse(link)
                path_parts = [p for p in parsed_url.path.split('/') if p]
                if len(path_parts) >= 2: # è‡³å°‘æœ‰ owner/repo
                    logging.info(f"âœ… Google æœå°‹æ‰¾åˆ° GitHub é€£çµ: {link} (Query: {query})")
                    return link
        logging.warning(f"ğŸ” Google æœå°‹æœªæ‰¾åˆ°ç›¸é—œçš„ GitHub å„²å­˜åº«é€£çµ (Query: {query})")
    else:
        logging.info(f"ğŸ” Google æœå°‹ç„¡çµæœ (Query: {query})")
    return None


def extract_github_owner_repo(repo_url):
    """å¾ GitHub URL ä¸­æå– owner å’Œ repository åç¨±ï¼Œä¸¦è™•ç†çµ„ç¹” URLã€‚"""
    if not repo_url:
        return None, None
    parsed_url = urlparse(repo_url)
    path_parts = [p for p in parsed_url.path.split('/') if p]

    if len(path_parts) >= 2:
        owner = path_parts[0]
        repo = path_parts[1]
        logging.info(f"âœ… æˆåŠŸå¾ URL æå– owner:{owner}, repo:{repo} from {repo_url}")
        return owner, repo
    elif len(path_parts) == 1:
        owner = path_parts[0]
        # å¦‚æœåªæœ‰ ownerï¼Œå˜—è©¦æŸ¥æ‰¾è©² owner ä¸‹æœ€æœ‰å¯èƒ½çš„ repo
        logging.warning(f"âš ï¸ åƒ…æ‰¾åˆ° GitHub owner: {owner} from {repo_url}ã€‚å˜—è©¦æŸ¥æ‰¾æœ€å¯èƒ½çš„å„²å­˜åº«...")
        return find_most_likely_repo(owner)
    
    logging.warning(f"âŒ ç„¡æ³•å¾ URL æå– owner/repo (æ ¼å¼ä¸ç¬¦): {repo_url}")
    return None, None

def find_most_likely_repo(owner):
    """å˜—è©¦å¾ GitHub çµ„ç¹”æˆ–ç”¨æˆ¶é é¢æ‰¾åˆ° commit æ•¸æœ€é«˜çš„å„²å­˜åº«ã€‚"""

    # å˜—è©¦ä½œç‚ºç”¨æˆ¶æŸ¥æ‰¾
    # GitHub API doesn't directly support sorting by commit count for repo listings.
    # We'll fetch all repos and then individually check commit counts.
    # For simplicity, we'll fetch a reasonable number of repos to check.
    # A more robust solution might involve pagination if there are many repos.
    url = f"https://api.github.com/users/{owner}/repos?per_page=100" # Fetch up to 100 repos
    repos_data = fetch_with_retry(url, GITHUB_HEADERS)

    if repos_data and isinstance(repos_data, list):
        max_commits = -1
        most_committed_repo = None

        for repo in repos_data:
            repo_name = repo['name']
            commits_url = f"https://api.github.com/repos/{owner}/{repo_name}/commits?per_page=1"
            # We only need the 'Link' header to get the total number of commits
            commits_response_headers = fetch_with_retry(commits_url, GITHUB_HEADERS, return_headers=True)

            if commits_response_headers and 'Link' in commits_response_headers:
                link_header = commits_response_headers['Link']
                # Extract the last page number from the Link header
                import re
                last_page_match = re.search(r'page=(\d+)>; rel="last"', link_header)
                if last_page_match:
                    total_commits = int(last_page_match.group(1))
                    if total_commits > max_commits:
                        max_commits = total_commits
                        most_committed_repo = repo_name
            else:
                # If no 'Link' header or other issue, try fetching the first page of commits
                # and count them. This is less efficient but a fallback.
                commits_data = fetch_with_retry(commits_url, GITHUB_HEADERS)
                if commits_data and isinstance(commits_data, list):
                    # If we can't get the total from 'Link', we'll assume the count on the first page
                    # is the total if per_page=1 is used and we are only looking for one commit.
                    # This is a simplification and might not be accurate for very large repos.
                    # For a truly accurate count without 'Link' header, you'd need to paginate
                    # through all commits.
                    # For this purpose, we assume if the first commit is returned, there's at least one.
                    if len(commits_data) > 0:
                        # This fallback is imperfect as it doesn't give total count.
                        # It primarily serves to check if there are *any* commits if 'Link' fails.
                        # We'll skip this fallback for now as it makes the logic complex for true "max commits".
                        pass

        if most_committed_repo:
            logging.info(f"âœ… æ‰¾åˆ°ç”¨æˆ¶ {owner} commit æ•¸æœ€é«˜çš„å„²å­˜åº«: {most_committed_repo} (ç¸½æäº¤æ•¸: {max_commits})")
            return owner, most_committed_repo

    # å˜—è©¦ä½œç‚ºçµ„ç¹”æŸ¥æ‰¾ (èˆ‡ç”¨æˆ¶é‚è¼¯ç›¸åŒ)
    url = f"https://api.github.com/orgs/{owner}/repos?per_page=100" # Fetch up to 100 repos
    repos_data = fetch_with_retry(url, GITHUB_HEADERS)

    if repos_data and isinstance(repos_data, list):
        max_commits = -1
        most_committed_repo = None

        for repo in repos_data:
            repo_name = repo['name']
            commits_url = f"https://api.github.com/repos/{owner}/{repo_name}/commits?per_page=1"
            commits_response_headers = fetch_with_retry(commits_url, GITHUB_HEADERS, return_headers=True)

            if commits_response_headers and 'Link' in commits_response_headers:
                link_header = commits_response_headers['Link']
                import re
                last_page_match = re.search(r'page=(\d+)>; rel="last"', link_header)
                if last_page_match:
                    total_commits = int(last_page_match.group(1))
                    if total_commits > max_commits:
                        max_commits = total_commits
                        most_committed_repo = repo_name

        if most_committed_repo:
            logging.info(f"âœ… æ‰¾åˆ°çµ„ç¹” {owner} commit æ•¸æœ€é«˜çš„å„²å­˜åº«: {most_committed_repo} (ç¸½æäº¤æ•¸: {max_commits})")
            return owner, most_committed_repo

    logging.warning(f"âŒ ç„¡æ³•ç‚º owner: {owner} æ‰¾åˆ° commit æ•¸æœ€é«˜çš„å„²å­˜åº«ã€‚")
    return None, None

def fetch_github_repo_stats(owner, repo):
    """å¾ GitHub API ç²å–å„²å­˜åº«çš„æäº¤æ´»å‹•ã€æ˜Ÿæ•¸å’Œ Fork æ•¸ã€‚"""
    stats_url = f"https://api.github.com/repos/{owner}/{repo}/stats/commit_activity"
    repo_info_url = f"https://api.github.com/repos/{owner}/{repo}"

    logging.info(f"å˜—è©¦ç²å–æäº¤æ´»å‹•: {stats_url} å»¶é²10ç§’é¿å…é™æµ")
    time.sleep(random.uniform(10, 15))
    commit_activity = fetch_with_retry(stats_url, GITHUB_HEADERS)
    
    # è™•ç† 404 Not Found çš„æƒ…æ³
    if commit_activity is None:
        logging.warning(f"âŒ ç„¡æ³•ç²å– {owner}/{repo} çš„æäº¤æ´»å‹•ï¼Œå¯èƒ½å„²å­˜åº«ä¸å­˜åœ¨æˆ–ç§æœ‰ã€‚")
        return None, None, None

    # å°‡æ¯é€±çš„æäº¤æ•¸æå–å‡ºä¾†
    commits_per_week = [week['total'] for week in commit_activity] if commit_activity else []
    
    logging.info(f"å˜—è©¦ç²å–å„²å­˜åº«è³‡è¨Š: {repo_info_url}")
    repo_info = fetch_with_retry(repo_info_url, GITHUB_HEADERS)
    
    stars = repo_info.get('stargazers_count') if repo_info else None
    forks = repo_info.get('forks_count') if repo_info else None

    if not commits_per_week or stars is None or forks is None:
        logging.warning(f"âŒ æœªèƒ½ç²å– {owner}/{repo} çš„æ‰€æœ‰å¿…è¦çµ±è¨ˆæ•¸æ“šã€‚")
        return None, None, None

    return commits_per_week, stars, forks

def calc_commit_growth(commits_per_week):
    """è¨ˆç®— GitHub æäº¤æ´»å‹•çš„æˆé•·ç‡ã€‚"""
    if len(commits_per_week) < 30: # è‡³å°‘éœ€è¦ 30 é€±æ•¸æ“šä¾†è¨ˆç®—å‰å¾Œ 15 é€±
        logging.warning(f"æ•¸æ“šä¸è¶³ (åªæœ‰ {len(commits_per_week)} é€±)ï¼Œç„¡æ³•è¨ˆç®—æˆé•·ç‡ã€‚")
        return None

    recent_15w = sum(commits_per_week[-15:])
    early_15w = sum(commits_per_week[-30:-15])

    if early_15w == 0:
        if recent_15w == 0:
            return 0.0
        else:
            return float('inf') # é¿å…é™¤ä»¥é›¶ï¼Œè¡¨ç¤ºç„¡é™æˆé•·
    
    growth = ((recent_15w - early_15w) / early_15w) * 100
    return growth

def load_processed_symbols(csv_filename):
    """è¼‰å…¥å·²è™•ç†çš„å¹£ç¨®åˆ—è¡¨ã€‚"""
    if os.path.exists(csv_filename):
        df = pd.read_csv(csv_filename)
        return set(df['symbol'].unique())
    return set()

def update_success_file(symbol):
    """æ›´æ–°æˆåŠŸè™•ç†çš„å¹£ç¨®åˆ—è¡¨ã€‚"""
    with open(SUCCESS_FILE, 'a', encoding='utf-8') as f:
        f.write(symbol + '\n')

def load_success_symbols():
    """è¼‰å…¥å·²æˆåŠŸè™•ç†çš„å¹£ç¨®ã€‚"""
    if os.path.exists(SUCCESS_FILE):
        with open(SUCCESS_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    return set()

def main():
    # ç¢ºä¿ CSV æª”æ¡ˆå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºå¸¶æœ‰æ¨™é ­çš„æª”æ¡ˆ
    csv_filename = CSV_FILE
    file_exists = os.path.exists(csv_filename)
    if not file_exists:
        # å®šç¾© CSV æª”æ¡ˆçš„æ¨™é ­
        header = ['coin_id', 'symbol', 'repo', 'growth_%', 'recent_15w', 'early_15w', 'stars', 'forks']
        df_empty = pd.DataFrame(columns=header)
        df_empty.to_csv(csv_filename, index=False, encoding='utf-8')
        logging.info(f"å‰µå»ºæ–°çš„ CSV æª”æ¡ˆ: {csv_filename}")

    # è¼‰å…¥å·²è™•ç†éçš„å¹£ç¨®åˆ—è¡¨
    processed_symbols_in_csv = load_processed_symbols(csv_filename)
    logging.info(f"å¾ {csv_filename} è¼‰å…¥ {len(processed_symbols_in_csv)} å€‹å·²è™•ç†ç¬¦è™Ÿã€‚")

    # ç²å–ç†±é–€å¹£ç¨®
    top_symbols = get_top_symbols(limit=100)
    if not top_symbols:
        logging.error("æœªèƒ½ç²å–ç†±é–€å¹£ç¨®åˆ—è¡¨ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        return

    logging.info(f"æ‰¾åˆ° {len(top_symbols)} å€‹ç†±é–€å¹£ç¨®ç¬¦è™Ÿ å‰äº”ç‚º:{top_symbols[:5]}")

    for symbol in tqdm(top_symbols, desc="è™•ç†å¹£ç¨®"):
        if symbol in processed_symbols_in_csv:
            logging.info(f"æ‰¾åˆ° {symbol} å­˜åœ¨csvä¸­è·³é")
            continue

        logging.info(f"{symbol} ä¸å­˜åœ¨csvä¸­ é–‹å§‹è·‘åˆ†æ å˜—è©¦å–å¾—CoinGecko ID")
        coingecko_id = get_coingecko_id(symbol)
        if not coingecko_id:
            logging.warning(f"âŒ ç„¡æ³•æ‰¾åˆ° {symbol} çš„ CoinGecko IDï¼Œè·³éã€‚")
            continue
        logging.info(f"æ‰¾åˆ° {symbol} çš„ CoinGecko ID:{coingecko_id}")

        # 1. å˜—è©¦å¾ CoinGecko ç²å– GitHub URL
        repo_url = fetch_coingecko_github_repo_url(coingecko_id)
        
        # 2. å¦‚æœ CoinGecko æ²’æœ‰æä¾›ï¼Œå˜—è©¦ Google æœå°‹
        if not repo_url and GOOGLE_API_KEY and GOOGLE_CSE_ID:
            logging.info(f"CoinGecko æœªæ‰¾åˆ° {coingecko_id} çš„ GitHub é€£çµï¼Œå˜—è©¦ Google æœå°‹...")
            # å˜—è©¦ç”¨ CoinGecko ID å’Œ å¹£ç¨®ç¬¦è™Ÿé€²è¡Œæœå°‹
            repo_url = Google_Search_github_repo(f"{coingecko_id} github")
            if not repo_url:
                repo_url = Google_Search_github_repo(f"{symbol} github")
            if not repo_url:
                logging.warning(f"âŒ ç„¡æ³•å¾ Google æœå°‹æ‰¾åˆ° {coingecko_id} æˆ– {symbol} çš„ GitHub repo é€£çµã€‚")
        
        if not repo_url:
            logging.warning(f"âŒ ç„¡æ³•æ‰¾åˆ° {coingecko_id} çš„ GitHub repo é€£çµ")
            continue

        logging.info(f"æ‰¾åˆ° {coingecko_id} çš„ GitHub repo é€£çµ:{repo_url}")

        # 3. æå– owner å’Œ repo
        owner, repo = extract_github_owner_repo(repo_url)
        if not owner or not repo:
            logging.warning(f"âŒ ç„¡æ³•ç²å– {coingecko_id} çš„ GitHub owner/repo è³‡è¨Š repo_url:{repo_url}")
            continue
        
        # 4. æ ¹æ“š owner, repo æ‹¿åˆ° commits, stars, forks
        commits, stars, forks = fetch_github_repo_stats(owner, repo)
        if commits is None:
            logging.warning(f"æ‹¿å– {coingecko_id} çš„ commits, stars, forks å¤±æ•—")
            continue
        logging.info(f"æ‹¿å– {coingecko_id} çš„ commits, stars, forks æˆåŠŸ")
        
        # 5. è¨ˆç®— commit æˆé•·
        growth = calc_commit_growth(commits)
        if growth is None:
            logging.warning(f"è¨ˆç®— growth å¤±æ•— coingecko_id:{coingecko_id}")
            continue
        logging.info(f"è¨ˆç®— growth æˆåŠŸ coingecko_id:{coingecko_id}")
        
        # 6. æ–°å¢ä¸€ç­†ç´€éŒ„
        new_record = [{
            'coin_id': coingecko_id,
            'symbol': symbol,
            'repo': f'{owner}/{repo}',
            'growth_%': growth,
            'recent_15w': sum(commits[-15:]),
            'early_15w': sum(commits[-30:-15]), # ä¿®æ­£ç‚ºæ­£ç¢ºçš„æ—©æœŸ15é€±
            'stars': stars,
            'forks': forks,
        }]

        if new_record:
            new_df = pd.DataFrame(new_record)
            new_df.to_csv(csv_filename, mode='a', header=False, index=False, encoding='utf-8') # header=False å› ç‚ºç¬¬ä¸€æ¬¡å·²å¯«å…¥
            processed_symbols_in_csv.add(symbol) # å°‡æ–°è™•ç†çš„ç¬¦è™Ÿæ·»åŠ åˆ°å·²è™•ç†é›†åˆä¸­
            time.sleep(random.uniform(1, 3)) # å¢åŠ éš¨æ©Ÿå»¶é²ï¼Œé¿å…éå¿«è«‹æ±‚
            logging.info(f"âœ… æˆåŠŸå°‡ {symbol} çš„æ•¸æ“šå¯«å…¥ {csv_filename}")

    logging.info("æ‰€æœ‰ç†±é–€å¹£ç¨®è™•ç†å®Œç•¢ã€‚")

if __name__ == "__main__":
    main()